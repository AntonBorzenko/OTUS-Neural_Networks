{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from utils import mnist, plot_graphs, mnist_transform\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = mnist(valid=10000)\n",
    "\n",
    "train_length = 1000\n",
    "train_data = datasets.MNIST('./MNIST_data', train=True, download=True, transform=mnist_transform)\n",
    "train_loader = DataLoader(train_data, batch_size=50, sampler=SubsetRandomSampler( np.arange(train_length) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, batchnorm=False, dropout=False, lr=1e-4, l2=0.):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        if batchnorm:\n",
    "            self.bn = nn.BatchNorm1d(128)\n",
    "        self.batchnorm = batchnorm\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.optim = optim.Adam(self.parameters(), lr=lr, weight_decay=l2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        if self.batchnorm:\n",
    "            x = self.bn(x)\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        if self.dropout:\n",
    "            x = F.dropout(x, 0.5)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, output, target, **kwargs):\n",
    "        self._loss = F.nll_loss(output, target, **kwargs)\n",
    "        return self._loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, models):\n",
    "    train_size = len(train_loader.sampler)\n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        for k, model in models.items():\n",
    "            model.optim.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = model.loss(output, target)\n",
    "            loss.backward()\n",
    "            model.optim.step()\n",
    "            \n",
    "        if batch_idx % 200 == 0:\n",
    "            line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "                epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(train_loader))\n",
    "            losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "            print(line + losses)\n",
    "            \n",
    "    else:\n",
    "        batch_idx += 1\n",
    "        line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "            epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(train_loader))\n",
    "        losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "        print(line + losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, loader, log=None):\n",
    "    test_size = len(loader.sampler)\n",
    "    \n",
    "    avg_lambda = lambda l: 'Loss: {:.4f}'.format(l)\n",
    "    acc_lambda = lambda c, p: 'Accuracy: {}/{} ({:.0f}%)'.format(c, test_size, p)\n",
    "    line = lambda i, l, c, p: '{}: '.format(i) + avg_lambda(l) + '\\t' + acc_lambda(c, p)\n",
    "\n",
    "    test_loss = {k: 0. for k in models}\n",
    "    correct = {k: 0. for k in models}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            # output = {k: m(data) for m in models}\n",
    "            for k, m in models.items():\n",
    "                output = m(data)\n",
    "                test_loss[k] += m.loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "                pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "                correct[k] += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "    \n",
    "    for k in models:\n",
    "        test_loss[k] /= test_size\n",
    "    correct_pct = {k: 100. * correct[k] / test_size for k in correct}\n",
    "    lines = '\\n'.join([line(k, test_loss[k], correct[k], correct_pct[k]) for k in models]) + '\\n'\n",
    "    report = 'Test set:\\n' + lines\n",
    "    if log is not None:\n",
    "        for k in models:\n",
    "            log[k].append((test_loss[k], correct_pct[k]))\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test(train_log, test_log):\n",
    "    data = {}\n",
    "    for name, model in train_log.items():\n",
    "        train_name = '{}_train'.format(name)\n",
    "        test_name = '{}_test'.format(name)\n",
    "        data[train_name] = model\n",
    "        data[test_name] = test_log[name]\n",
    "    \n",
    "    plot_graphs(data, 'loss')\n",
    "    plot_graphs(data, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = {'default': Net(False, False), 'bn': Net(True, False), 'drop': Net(False, True), 'both': Net(True, True)}\n",
    "models = {'default': Net(False, True, lr=1e-3)}\n",
    "train_log = {k: [] for k in models}\n",
    "test_log = {k: [] for k in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1000 (0%)]\tLosses default: 2.341486\n",
      "Train Epoch: 0 [1000/1000 (100%)]\tLosses default: 1.085487\n",
      "Test set:\n",
      "default: Loss: 1.0592\tAccuracy: 763.0/1000 (76%)\n",
      "\n",
      "Test set:\n",
      "default: Loss: 1.1474\tAccuracy: 7134.0/10000 (71%)\n",
      "\n",
      "Train Epoch: 1 [0/1000 (0%)]\tLosses default: 1.134236\n",
      "Train Epoch: 1 [1000/1000 (100%)]\tLosses default: 0.561197\n",
      "Test set:\n",
      "default: Loss: 0.5820\tAccuracy: 874.0/1000 (87%)\n",
      "\n",
      "Test set:\n",
      "default: Loss: 0.7110\tAccuracy: 8105.0/10000 (81%)\n",
      "\n",
      "Train Epoch: 2 [0/1000 (0%)]\tLosses default: 0.384903\n",
      "Train Epoch: 2 [1000/1000 (100%)]\tLosses default: 0.405963\n",
      "Test set:\n",
      "default: Loss: 0.3809\tAccuracy: 909.0/1000 (91%)\n",
      "\n",
      "Test set:\n",
      "default: Loss: 0.5465\tAccuracy: 8450.0/10000 (84%)\n",
      "\n",
      "Train Epoch: 3 [0/1000 (0%)]\tLosses default: 0.627154\n",
      "Train Epoch: 3 [1000/1000 (100%)]\tLosses default: 0.318451\n",
      "Test set:\n",
      "default: Loss: 0.2862\tAccuracy: 941.0/1000 (94%)\n",
      "\n",
      "Test set:\n",
      "default: Loss: 0.4794\tAccuracy: 8626.0/10000 (86%)\n",
      "\n",
      "Train Epoch: 4 [0/1000 (0%)]\tLosses default: 0.221927\n",
      "Train Epoch: 4 [1000/1000 (100%)]\tLosses default: 0.199763\n",
      "Test set:\n",
      "default: Loss: 0.2134\tAccuracy: 963.0/1000 (96%)\n",
      "\n",
      "Test set:\n",
      "default: Loss: 0.4568\tAccuracy: 8604.0/10000 (86%)\n",
      "\n",
      "Train Epoch: 5 [0/1000 (0%)]\tLosses default: 0.138254\n",
      "Train Epoch: 5 [1000/1000 (100%)]\tLosses default: 0.135194\n",
      "Test set:\n",
      "default: Loss: 0.1655\tAccuracy: 973.0/1000 (97%)\n",
      "\n",
      "Test set:\n",
      "default: Loss: 0.4367\tAccuracy: 8680.0/10000 (87%)\n",
      "\n",
      "Train Epoch: 6 [0/1000 (0%)]\tLosses default: 0.130894\n",
      "Train Epoch: 6 [1000/1000 (100%)]\tLosses default: 0.095434\n",
      "Test set:\n",
      "default: Loss: 0.1320\tAccuracy: 981.0/1000 (98%)\n",
      "\n",
      "Test set:\n",
      "default: Loss: 0.4225\tAccuracy: 8697.0/10000 (87%)\n",
      "\n",
      "Train Epoch: 7 [0/1000 (0%)]\tLosses default: 0.138909\n",
      "Train Epoch: 7 [1000/1000 (100%)]\tLosses default: 0.144864\n",
      "Test set:\n",
      "default: Loss: 0.1147\tAccuracy: 982.0/1000 (98%)\n",
      "\n",
      "Test set:\n",
      "default: Loss: 0.4329\tAccuracy: 8656.0/10000 (87%)\n",
      "\n",
      "Train Epoch: 8 [0/1000 (0%)]\tLosses default: 0.050555\n",
      "Train Epoch: 8 [1000/1000 (100%)]\tLosses default: 0.110141\n",
      "Test set:\n",
      "default: Loss: 0.0824\tAccuracy: 988.0/1000 (99%)\n",
      "\n",
      "Test set:\n",
      "default: Loss: 0.4190\tAccuracy: 8727.0/10000 (87%)\n",
      "\n",
      "Train Epoch: 9 [0/1000 (0%)]\tLosses default: 0.065756\n",
      "Train Epoch: 9 [1000/1000 (100%)]\tLosses default: 0.123286\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for model in models.values():\n",
    "    for g in model.optim.param_groups:\n",
    "        g['lr'] = 1e-3\n",
    "\n",
    "for epoch in range(0, 30):\n",
    "    for model in models.values():\n",
    "        model.train()\n",
    "    train(epoch, models)\n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "    test(models, train_loader, train_log)\n",
    "    test(models, valid_loader, test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_test(train_log, test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
