{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math \n",
    "\n",
    "import sys\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_FIELD = torchtext.data.Field(lower=True, tokenize=lambda w: list(w))\n",
    "train_set, val_set, test_set = torchtext.datasets.WikiText2.splits(CHAR_FIELD)\n",
    "CHAR_FIELD.build_vocab(train_set)\n",
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits((train_set, val_set, test_set), batch_size=128, bptt_len=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LEN = len(CHAR_FIELD.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iter, ntokens=VOCAB_LEN):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch, data in enumerate(train_iter):\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data.text)\n",
    "        loss = criterion(output.view(-1, ntokens), data.target.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_iter), lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_iter, ntokens=VOCAB_LEN):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i, data in enumerate(data_iter):\n",
    "        output, hidden = model(data.text)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * criterion(output_flat, data.target.view(-1)).item()\n",
    "    return total_loss / len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, itos, n=50, temp=1., ntokens=VOCAB_LEN):\n",
    "    model.eval()\n",
    "    x = torch.rand(1, 1).mul(ntokens).long()\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "        s = itos[s_idx]\n",
    "        out.append(s)\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel('LSTM', VOCAB_LEN, 128, 128, 2, 0.3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "sequence_length = 30\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "log_interval = 100\n",
    "eval_batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " ยルμ−„礮⅓ệṯư°wュ-プpვệpż₤€ăäh大.o·ōス&þ̃هキ♭̃าê・ッ〉v>3v่รu \n",
      "\n",
      "| epoch   1 |   100/ 2808 batches | lr 4.00 | loss  3.46 | ppl    31.75\n",
      "| epoch   1 |   200/ 2808 batches | lr 4.00 | loss  3.15 | ppl    23.40\n",
      "| epoch   1 |   300/ 2808 batches | lr 4.00 | loss  3.12 | ppl    22.76\n",
      "| epoch   1 |   400/ 2808 batches | lr 4.00 | loss  3.10 | ppl    22.26\n",
      "| epoch   1 |   500/ 2808 batches | lr 4.00 | loss  3.10 | ppl    22.30\n",
      "| epoch   1 |   600/ 2808 batches | lr 4.00 | loss  3.09 | ppl    22.08\n",
      "| epoch   1 |   700/ 2808 batches | lr 4.00 | loss  3.06 | ppl    21.29\n",
      "| epoch   1 |   800/ 2808 batches | lr 4.00 | loss  2.90 | ppl    18.20\n",
      "| epoch   1 |   900/ 2808 batches | lr 4.00 | loss  2.77 | ppl    15.90\n",
      "| epoch   1 |  1000/ 2808 batches | lr 4.00 | loss  2.65 | ppl    14.14\n",
      "| epoch   1 |  1100/ 2808 batches | lr 4.00 | loss  2.54 | ppl    12.71\n",
      "| epoch   1 |  1200/ 2808 batches | lr 4.00 | loss  2.47 | ppl    11.86\n",
      "| epoch   1 |  1300/ 2808 batches | lr 4.00 | loss  2.42 | ppl    11.23\n",
      "| epoch   1 |  1400/ 2808 batches | lr 4.00 | loss  2.36 | ppl    10.56\n",
      "| epoch   1 |  1500/ 2808 batches | lr 4.00 | loss  2.32 | ppl    10.14\n",
      "| epoch   1 |  1600/ 2808 batches | lr 4.00 | loss  2.28 | ppl     9.79\n",
      "| epoch   1 |  1700/ 2808 batches | lr 4.00 | loss  2.25 | ppl     9.49\n",
      "| epoch   1 |  1800/ 2808 batches | lr 4.00 | loss  2.23 | ppl     9.27\n",
      "| epoch   1 |  1900/ 2808 batches | lr 4.00 | loss  2.21 | ppl     9.07\n",
      "| epoch   1 |  2000/ 2808 batches | lr 4.00 | loss  2.17 | ppl     8.80\n",
      "| epoch   1 |  2100/ 2808 batches | lr 4.00 | loss  2.16 | ppl     8.66\n",
      "| epoch   1 |  2200/ 2808 batches | lr 4.00 | loss  2.14 | ppl     8.49\n",
      "| epoch   1 |  2300/ 2808 batches | lr 4.00 | loss  2.13 | ppl     8.41\n",
      "| epoch   1 |  2400/ 2808 batches | lr 4.00 | loss  2.10 | ppl     8.19\n",
      "| epoch   1 |  2500/ 2808 batches | lr 4.00 | loss  2.09 | ppl     8.10\n",
      "| epoch   1 |  2600/ 2808 batches | lr 4.00 | loss  2.07 | ppl     7.96\n",
      "| epoch   1 |  2700/ 2808 batches | lr 4.00 | loss  2.06 | ppl     7.84\n",
      "| epoch   1 |  2800/ 2808 batches | lr 4.00 | loss  2.03 | ppl     7.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss 238.79 | valid ppl 50687268292169071194396557850590002352686917684812435481714685378472662040624835243050911261593492783104.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  mil ellamaed ctile , the šancpols the cas hemter  \n",
      "\n",
      "| epoch   2 |   100/ 2808 batches | lr 4.00 | loss  2.05 | ppl     7.73\n",
      "| epoch   2 |   200/ 2808 batches | lr 4.00 | loss  2.01 | ppl     7.43\n",
      "| epoch   2 |   300/ 2808 batches | lr 4.00 | loss  2.00 | ppl     7.37\n",
      "| epoch   2 |   400/ 2808 batches | lr 4.00 | loss  1.99 | ppl     7.29\n",
      "| epoch   2 |   500/ 2808 batches | lr 4.00 | loss  1.97 | ppl     7.20\n",
      "| epoch   2 |   600/ 2808 batches | lr 4.00 | loss  1.97 | ppl     7.14\n",
      "| epoch   2 |   700/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.07\n",
      "| epoch   2 |   800/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.01\n",
      "| epoch   2 |   900/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.97\n",
      "| epoch   2 |  1000/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.91\n",
      "| epoch   2 |  1100/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.81\n",
      "| epoch   2 |  1200/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch   2 |  1300/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.72\n",
      "| epoch   2 |  1400/ 2808 batches | lr 4.00 | loss  1.88 | ppl     6.58\n",
      "| epoch   2 |  1500/ 2808 batches | lr 4.00 | loss  1.89 | ppl     6.60\n",
      "| epoch   2 |  1600/ 2808 batches | lr 4.00 | loss  1.88 | ppl     6.57\n",
      "| epoch   2 |  1700/ 2808 batches | lr 4.00 | loss  1.87 | ppl     6.52\n",
      "| epoch   2 |  1800/ 2808 batches | lr 4.00 | loss  1.87 | ppl     6.50\n",
      "| epoch   2 |  1900/ 2808 batches | lr 4.00 | loss  1.87 | ppl     6.51\n",
      "| epoch   2 |  2000/ 2808 batches | lr 4.00 | loss  1.86 | ppl     6.40\n",
      "| epoch   2 |  2100/ 2808 batches | lr 4.00 | loss  1.86 | ppl     6.41\n",
      "| epoch   2 |  2200/ 2808 batches | lr 4.00 | loss  1.85 | ppl     6.37\n",
      "| epoch   2 |  2300/ 2808 batches | lr 4.00 | loss  1.85 | ppl     6.37\n",
      "| epoch   2 |  2400/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.26\n",
      "| epoch   2 |  2500/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.25\n",
      "| epoch   2 |  2600/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.26\n",
      "| epoch   2 |  2700/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.22\n",
      "| epoch   2 |  2800/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss 207.04 | valid ppl 824498696887828096052994857391906997946033564787867010370919426080635108930613765621153792.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " to poem and the was the for the death . the dafo 2 \n",
      "\n",
      "| epoch   3 |   100/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.23\n",
      "| epoch   3 |   200/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.07\n",
      "| epoch   3 |   300/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.07\n",
      "| epoch   3 |   400/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.05\n",
      "| epoch   3 |   500/ 2808 batches | lr 4.00 | loss  1.79 | ppl     6.01\n",
      "| epoch   3 |   600/ 2808 batches | lr 4.00 | loss  1.79 | ppl     5.99\n",
      "| epoch   3 |   700/ 2808 batches | lr 4.00 | loss  1.79 | ppl     5.98\n",
      "| epoch   3 |   800/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.95\n",
      "| epoch   3 |   900/ 2808 batches | lr 4.00 | loss  1.79 | ppl     5.96\n",
      "| epoch   3 |  1000/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.93\n",
      "| epoch   3 |  1100/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.88\n",
      "| epoch   3 |  1200/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.90\n",
      "| epoch   3 |  1300/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.87\n",
      "| epoch   3 |  1400/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.76\n",
      "| epoch   3 |  1500/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.80\n",
      "| epoch   3 |  1600/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.81\n",
      "| epoch   3 |  1700/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.77\n",
      "| epoch   3 |  1800/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.78\n",
      "| epoch   3 |  1900/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.82\n",
      "| epoch   3 |  2000/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
      "| epoch   3 |  2100/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.78\n",
      "| epoch   3 |  2200/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.75\n",
      "| epoch   3 |  2300/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.76\n",
      "| epoch   3 |  2400/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
      "| epoch   3 |  2500/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
      "| epoch   3 |  2600/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.71\n",
      "| epoch   3 |  2700/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
      "| epoch   3 |  2800/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss 195.21 | valid ppl 5999980973197066861608871961282607472032077635419533985500289983055699250444477923328.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " reslesfing the later a new \" . a sonals of doning  \n",
      "\n",
      "| epoch   4 |   100/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.73\n",
      "| epoch   4 |   200/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   4 |   300/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "| epoch   4 |   400/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "| epoch   4 |   500/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   4 |   600/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   4 |   700/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   4 |   800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   4 |   900/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch   4 |  1000/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   4 |  1100/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.51\n",
      "| epoch   4 |  1200/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  1300/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.50\n",
      "| epoch   4 |  1400/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.41\n",
      "| epoch   4 |  1500/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "| epoch   4 |  1600/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "| epoch   4 |  1700/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "| epoch   4 |  1800/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "| epoch   4 |  1900/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
      "| epoch   4 |  2000/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.44\n",
      "| epoch   4 |  2100/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.49\n",
      "| epoch   4 |  2200/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "| epoch   4 |  2300/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "| epoch   4 |  2400/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.40\n",
      "| epoch   4 |  2500/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.41\n",
      "| epoch   4 |  2600/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "| epoch   4 |  2700/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.42\n",
      "| epoch   4 |  2800/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss 189.35 | valid ppl 17108019616376672390717522663152644574257019130431873996659447917055800229511561216.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  eastern 's rhodan for len extending a some ea maj \n",
      "\n",
      "| epoch   5 |   100/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "| epoch   5 |   200/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.34\n",
      "| epoch   5 |   300/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.38\n",
      "| epoch   5 |   400/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.38\n",
      "| epoch   5 |   500/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.36\n",
      "| epoch   5 |   600/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.34\n",
      "| epoch   5 |   700/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.36\n",
      "| epoch   5 |   800/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.34\n",
      "| epoch   5 |   900/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.35\n",
      "| epoch   5 |  1000/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.34\n",
      "| epoch   5 |  1100/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.32\n",
      "| epoch   5 |  1200/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.35\n",
      "| epoch   5 |  1300/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.31\n",
      "| epoch   5 |  1400/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.23\n",
      "| epoch   5 |  1500/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.29\n",
      "| epoch   5 |  1600/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.29\n",
      "| epoch   5 |  1700/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.27\n",
      "| epoch   5 |  1800/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.29\n",
      "| epoch   5 |  1900/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.34\n",
      "| epoch   5 |  2000/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.27\n",
      "| epoch   5 |  2100/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.32\n",
      "| epoch   5 |  2200/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.30\n",
      "| epoch   5 |  2300/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.31\n",
      "| epoch   5 |  2400/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.24\n",
      "| epoch   5 |  2500/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.25\n",
      "| epoch   5 |  2600/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.29\n",
      "| epoch   5 |  2700/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.27\n",
      "| epoch   5 |  2800/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss 185.92 | valid ppl 553901192521342552719405478816278989768439551827094500721075210344896110488715264.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  with the formulth matter ) on norcessiona time th \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(model, CHAR_FIELD.vocab.itos, 50), '\\n')\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train(model, train_iter)\n",
    "    val_loss = evaluate(model, val_iter)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(model, CHAR_FIELD.vocab.itos, 50), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss = 1.45, val ppl = 4.27\n"
     ]
    }
   ],
   "source": [
    "print(\"Val loss = {:.2f}, val ppl = {:.2f}\".format(val_loss / 128, math.exp(val_loss / 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
